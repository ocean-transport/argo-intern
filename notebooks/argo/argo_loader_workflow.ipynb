{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argo Data Loading Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gsw\n",
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher\n",
    "argo_loader = ArgoDataFetcher(src=\"gdac\", ftp=\"/swot/SUM05/dbalwada/Argo_sync\", progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've synced a local copy of the entire Argo dataset that lives in the directory `/swot/SUM05/dbalwada/Argo_sync`. Instructions for how to do this can be found here: http://www.argodatamgt.org/Access-to-data/Argo-GDAC-synchronization-service\n",
    "\n",
    "It's also possible to download from a remote server directly (like using `src=\"erddap\"`), however I've found this to be inefficient and error-prone for large regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Small Box (~5deg x 5deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads the Argo data in a given lat/lon range and returns it as one dataset, interpolated to a common pressure grid. This is suitable for fairly small regions (a few degrees each direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(box, standard_grid=np.arange(0,2002,2)):\n",
    "    \"\"\"Takes lat, lon parameters and a pressure grid and returns an xr ds \n",
    "    with CT, SA, SIG0, SPICE, and sample_rate interpolated to that grid.\n",
    "\n",
    "    box: (list) of the form [lon_min, lon_max, lat_min, lat_max, z_min, z_max]\n",
    "    standard_grid: (list) pressure grid for interpolation\n",
    "    \"\"\"\n",
    "\n",
    "    ds = argo_loader.region(box)\n",
    "    print(\"loading points complete\")\n",
    "\n",
    "    ds = ds.to_xarray()\n",
    "    print(\"to xarray complete\")\n",
    "\n",
    "    ds = ds.argo.teos10([\"CT\", \"SA\", \"SIG0\"])\n",
    "    ds = ds.argo.point2profile()\n",
    "    print(\"point to profile complete\")\n",
    "\n",
    "    ds_interp = get_ds_interp(ds, standard_grid)\n",
    "    print(\"interpolation complete\")\n",
    "\n",
    "    ds_interp[\"SPICE\"] = gsw.spiciness0(ds_interp.SA, ds_interp.CT).rename(\"SPICE\")\n",
    "    print(\"adding spice complete\")\n",
    "    \n",
    "    if 'raw_attrs' in ds_interp.attrs:\n",
    "        del ds_interp.attrs['raw_attrs']\n",
    "\n",
    "    return ds_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_interp(ds, standard_grid):\n",
    "    \"\"\"\n",
    "    Iterates profile by profile through ds, interpolating each to the\n",
    "    provided pressure grid. If the sampled profile does not reach the \n",
    "    extent of the pressure grid, it's filled in with NaNs.\n",
    "    Returns an xr ds with variables interpolated to a standard pressure\n",
    "    grid\n",
    "    \n",
    "    ds: (xr ds) with dimension of profiles (`N_PROF`)\n",
    "    standard_grid: (list) pressure grid for interpretation\n",
    "    \"\"\"\n",
    "    \n",
    "    print('NEW INTERP FUNCTION')\n",
    "    profs_interp = []\n",
    "    interp_step = standard_grid[1] - standard_grid[0]\n",
    "    \n",
    "    for n in range(0, len(ds.N_PROF)):\n",
    "        prof = ds.isel(N_PROF=n).expand_dims('N_PROF')\n",
    "        depth_min = int(prof.PRES.min())\n",
    "        depth_min = np.ceil(depth_min / 2) * 2\n",
    "        depth_max = int(prof.PRES.max())\n",
    "        depth_max = (depth_max // 2) * 2\n",
    "\n",
    "        if not (np.all(np.diff(standard_grid) > 0) and np.all(standard_grid >= 0)):\n",
    "            print(f\"\\tProfile {n} skipped due to invalid standard_grid values.\")\n",
    "            continue\n",
    "\n",
    "        if depth_max > depth_min:\n",
    "            dp = prof.PRES.diff('N_LEVELS')\n",
    "            prof['sample_rate'] = dp\n",
    "            \n",
    "            try:\n",
    "                prof_interp = prof.argo.interp_std_levels(np.arange(depth_min, depth_max, interp_step))\n",
    "                prof_interp_reindexed = prof_interp.reindex({'PRES_INTERPOLATED': standard_grid}, method=None, fill_value=np.nan)\n",
    "                profs_interp.append(prof_interp_reindexed)\n",
    "            except ValueError as e:\n",
    "                print(f\"\\tProfile {n} skipped due to interpolation error: {e}\")\n",
    "        \n",
    "        elif depth_max > prof.PRES.max():\n",
    "            print(f\"\\tProfile {n} has depth_max of {depth_max} but max PRES is {prof.PRES.max()}\")\n",
    "            \n",
    "        elif depth_max <= depth_min:\n",
    "            print(f\"\\tProfile {n} has invalid depth range: depth_min={depth_min}, depth_max={depth_max}\")\n",
    "\n",
    "    concat_n_prof = xr.concat(profs_interp, dim='N_PROF') if profs_interp else xr.Dataset()\n",
    "    \n",
    "    return concat_n_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = [-30,-25,0,5,0,2000]\n",
    "ds_box = get_box(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Large Region (~entire basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads the Argo data in a much larger lat/long range and saves the data into small boxes, at a filepath specified in the `get_box_delayed` function. After this is complete, look to the concat section below to concatonate the individual xr ds for each box into one ds for the whole reigon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow parallelizes the loading process by taking one large region and dividing it into subregions, and dividing each subregion into boxes. It iterates through subregions, taking one at a time and dividing it into boxes. Each box is then passed to one core, which loads all argo files in the given lat/lon range (using the `get_box` function from above), and saving the data as an xr ds in a netcdf file. The boxes are loaded in parallel using dask. Once one subregion is completed, it moves onto the next and repeates these steps until the whole region has been complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It make take some thought and practice to pick the `region_step` and `box_step` intervals to match the resouces available to your machine. For example, for efficiency, the number of boxes in each region should be slightly lower than the number of available cores. For a system with 72 cores, I would plan to have ~60 boxes in each subregion. It would also be ideal to make sure the number of boxes in each subregion is less than the number of workers initialized in the dask cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of workers, threads, and memory can be changed to correspond to a given machine. Or remove them to use default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "cluster = LocalCluster(n_workers=64, threads_per_worker=1, memory_limit='60GiB')\n",
    "client = Client(cluster)\n",
    "print(cluster) #this prints URL for dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def get_box_delayed(*args, **kwargs):\n",
    "    try:\n",
    "        return prf.get_box(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        return type(e).__name__, str(e)\n",
    "\n",
    "def get_box_dask(boxes_list):\n",
    "    \n",
    "    boxes_list = [tuple([box]) for box in boxes_list]\n",
    "    tasks = [get_box_delayed(*args) for args in boxes_list]\n",
    "    results = dask.compute(*tasks)\n",
    "    errors=[]\n",
    "\n",
    "    for n, result in enumerate(results):\n",
    "        if isinstance(result, tuple) and isinstance(result[0], str):\n",
    "            error_type, error_message = result\n",
    "            print(\"Error in box {}: {} - {}\".format([boxes_list[n][0][0],boxes_list[n][0][1],boxes_list[n][0][2],boxes_list[n][0][3]], error_type, error_message))\n",
    "            errors.append([boxes_list[n][0][0],boxes_list[n][0][1],boxes_list[n][0][2],boxes_list[n][0][3], error_type, error_message])\n",
    "        else:\n",
    "            result.to_netcdf(\"/swot/SUM05/amf2288/sync-boxes/new_test/lon:({},{})_lat:({},{})_ds_z.nc\".format(boxes_list[n][0][0],boxes_list[n][0][1],boxes_list[n][0][2],boxes_list[n][0][3]))\n",
    "            print(\"Saved box {} of {}\".format(n+1, len(results)))\n",
    "            \n",
    "    return errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(box, step):\n",
    "    grid = []\n",
    "    lon_min, lon_max, lat_min, lat_max = box[0],box[1],box[2],box[3]\n",
    "    lat = lat_min\n",
    "    while lat < lat_max:\n",
    "        lon = lon_min\n",
    "        while lon < lon_max:\n",
    "            box_lat_max = min(lat + step, lat_max)\n",
    "            box_lon_max = min(lon + step, lon_max)\n",
    "            box = [lat, box_lat_max, lon, box_lon_max, box[4], box[5]]\n",
    "            grid.append(box)\n",
    "            lon += step\n",
    "        lat += step\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(area, region_step, target_step):\n",
    "    \n",
    "    regions = generate_grid(area, region_step)\n",
    "    print('-' * 50)\n",
    "    print(\"Cluster: {}\".format(cluster))\n",
    "    print('-' * 50)\n",
    "    print(\"THE REGIONS ARE {}\".format(regions))\n",
    "    print('-' * 50)\n",
    "    \n",
    "    errors_list = []\n",
    "    \n",
    "    for n,region in enumerate(regions):\n",
    "        boxes = generate_grid(region, target_step)\n",
    "        print('-' * 50)\n",
    "        print(\"REGION #{} OUT OF {} IS: {}\".format(n+1, len(regions), region))\n",
    "        print('-' * 50)\n",
    "        print(\"THE BOXES IN REGION #{} ARE {}\".format(n+1,boxes))\n",
    "        print('-' * 50)\n",
    "\n",
    "        errors = get_box_dask(boxes)\n",
    "        errors_list.append([errors])\n",
    "        \n",
    "        print('-' * 50)\n",
    "        print(\"COMPLETED REGION #{} OUT OF {}\".format(n+1,len(regions)))\n",
    "        print('-' * 50)\n",
    "        \n",
    "    return errors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_atl = [-75,25,-90,0,0,2000]\n",
    "region_step = 40\n",
    "box_step = 5\n",
    "interp_step = 2\n",
    "get_region(s_atl, region_step, box_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT DOES `interp_step`/`region_step` DO???????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the individual boxes saved by the `get_region` function and concatonates them into one file, which is then saved. There are two functions, with the choice of saving the output as a netcdf file or as a zarr store. Once this is complete (and you've checked it has worked properly), probably good to delete the directory with individual netcdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_netcdf(input_dir: str, first_dim: str, second_dim: str, output_dir: str, output_file: str):\n",
    "    input_path = Path(input_dir)\n",
    "    netcdf_files = list(input_path.glob(\"*.nc\"))\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Open datasets lazily with dask\n",
    "    datasets = [xr.open_dataset(str(file), chunks={}) for file in netcdf_files]\n",
    "    \n",
    "    # Concatenate along the first dimension\n",
    "    combined_first_dim = xr.concat(datasets, dim=first_dim)\n",
    "    \n",
    "    # Rechunk the data to ensure uniform chunk sizes\n",
    "    combined_rechunked = combined_first_dim.chunk({first_dim: 256, second_dim: 256})  # Adjust chunk sizes as needed\n",
    "    \n",
    "    # Save to NetCDF\n",
    "    output_file_path = output_path / output_file\n",
    "    with ProgressBar():\n",
    "        combined_rechunked.to_netcdf(output_file_path, compute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_zarr(input_dir: str, first_dim: str, second_dim: str, output_dir: str, output_file: str):\n",
    "    input_path = Path(input_dir)\n",
    "    netcdf_files = list(input_path.glob(\"*.nc\"))\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Open datasets lazily with dask\n",
    "    datasets = [xr.open_dataset(str(file), chunks={}) for file in netcdf_files]\n",
    "    \n",
    "    # Concatenate along the first dimension\n",
    "    combined_first_dim = xr.concat(datasets, dim=first_dim)\n",
    "    \n",
    "    # Rechunk the data to ensure uniform chunk sizes\n",
    "    combined_rechunked = combined_first_dim.chunk({first_dim: 256, second_dim: 256})  # Adjust chunk sizes as needed\n",
    "    \n",
    "    # Save to Zarr\n",
    "    output_file_path = output_path / output_file\n",
    "    with ProgressBar():\n",
    "        combined_rechunked.to_zarr(output_file_path, compute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"/swot/SUM05/amf2288/sync-boxes/new_test\"\n",
    "output_directory = \"/swot/SUM05/amf2288/sync-boxes\"\n",
    "output_netcdf = \"new_test.nc\"\n",
    "output_zarr = \"new_test.zarr\"\n",
    "first_dim = \"N_PROF\"\n",
    "second_dim = \"PRES_INTERPOLATED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_netcdf(input_directory, first_dim, second_dim, output_directory, output_netcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate_zarr(input_directory, first_dim, second_dim, output_directory, output_zarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = open_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Argo_Feb_25]",
   "language": "python",
   "name": "conda-env-Argo_Feb_25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
