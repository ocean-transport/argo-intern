{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argo File Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with newest snapshot\n",
    "# try with ftp that gmaze used in issue, probably going to be really slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gsw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "from cmocean import cm as cmo\n",
    "import xrft\n",
    "import pandas as pd\n",
    "import argopy\n",
    "from argopy import DataFetcher\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argopy: 1.0.0, xarray: 2024.2.0, dask: 2024.10.0\n"
     ]
    }
   ],
   "source": [
    "print('argopy: {}, xarray: {}, dask: {}'.format(argopy.__version__, xr.__version__, dask.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home.ufs/amf2288/argo-intern/funcs')\n",
    "import density_funcs as df\n",
    "import EV_funcs as ef\n",
    "import filt_funcs as ff\n",
    "import plot_funcs as pf\n",
    "import processing_funcs as prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'processing_funcs' from '/home/amf2288/argo-intern/funcs/processing_funcs.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(df)\n",
    "reload(ef)\n",
    "reload(ff)\n",
    "reload(pf)\n",
    "#reload(mf)\n",
    "reload(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing parallel loading with dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client= Client(processes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = [-27.5, -22.5, -5, 5, 0, 2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with argopy.set_options(parallel=client):\n",
    "    f = DataFetcher(src='gdac', ftp='/swot/SUM05/dbalwada/Argo_sync', progress=True).region(box)\n",
    "    print('%i chunks to process' % len(f.uri))\n",
    "    print(f)\n",
    "    ds = f.load().data#.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay the process is running on multiple GPUs, in this case 10-12 machines. However, obviously there are a ton of error messages---I'm not really sure what's going wrong. The box I've started with is really small, so I don't think there should be a memory error. Not sure where to go from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load by box (coordinate: N_PROF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box=[lon_min,lon_max,lat_min,lat_max,depth_min,depth_max]\n",
    "box_atl = [-25,-20,-70,70,0,2001]\n",
    "box_pac = [-180,-175,-70,70,0,2001]\n",
    "box_watl = [-60,-55,10,45,0,2001]\n",
    "box_wpac = [150,155,-5,50,0,2001]\n",
    "box_wind = [60,65,-65,25,0,2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW = [-180,0,0,90,0,2001]\n",
    "NE = [0,180,-90,0,0,2001]\n",
    "SW = [-180,0,-90,0,0,2001]\n",
    "SE = [0,-180,-90,0,0,2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = [-180,-90,  0,90,0,2001]\n",
    "box2 = [- 90,  0,  0,90,0,2001]\n",
    "box3 = [   0, 90,  0,90,0,2001]\n",
    "box4 = [  90,180,  0,90,0,2001]\n",
    "box5 = [-180,-90,-90, 0,0,2001]\n",
    "box6 = [- 90,  0,-90, 0,0,2001]\n",
    "box7 = [   0, 90,-90, 0,0,2001]\n",
    "box8 = [  90,180,-90, 0,0,2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pf.plot_box([NW,NE,SW,SE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading points complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home.ufs/amf2288/mambaforge-pypy3/envs/Argo_Nov_24/lib/python3.12/site-packages/argopy/data_fetchers/gdac_data.py:331: UserWarning: Found more than 50 files to load, this may take a while to process sequentially ! Consider using another data source (eg: 'erddap') or the 'parallel=True' option to improve processing time.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 481/1898 [27:08<1:36:47,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 502, message='Proxy Error', url='https://data-argo.ifremer.fr/dac/aoml/5905088/5905088_prof.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1898/1898 [1:43:30<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to xarray complete\n",
      "point to profile complete\n",
      "interpolation complete\n",
      "adding spice complete\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mprf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox3\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/amf2288/argo-intern/funcs/processing_funcs.py:69\u001b[0m, in \u001b[0;36mget_box\u001b[0;34m(box, interp_step)\u001b[0m\n\u001b[1;32m     66\u001b[0m ds_interp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPICE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gsw\u001b[38;5;241m.\u001b[39mspiciness0(ds_interp\u001b[38;5;241m.\u001b[39mSA, ds_interp\u001b[38;5;241m.\u001b[39mCT)\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPICE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding spice complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m ds_interp \u001b[38;5;241m=\u001b[39m \u001b[43mget_MLD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_interp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m ds_interp \u001b[38;5;241m=\u001b[39m add_times(ds_interp)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding MLD complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/amf2288/argo-intern/funcs/processing_funcs.py:168\u001b[0m, in \u001b[0;36mget_MLD\u001b[0;34m(ds, threshold, variable, dim1, dim2)\u001b[0m\n\u001b[1;32m    166\u001b[0m     SIG0_diff \u001b[38;5;241m=\u001b[39m SIG0_surface \u001b[38;5;241m+\u001b[39m threshold\n\u001b[1;32m    167\u001b[0m     MLD_ds \u001b[38;5;241m=\u001b[39m SIG0_surface\u001b[38;5;241m.\u001b[39mwhere(ds\u001b[38;5;241m.\u001b[39misel({dim1: n})[variable] \u001b[38;5;241m<\u001b[39m SIG0_diff)\n\u001b[0;32m--> 168\u001b[0m     MLD \u001b[38;5;241m=\u001b[39m \u001b[43mMLD_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mdim2\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[dim2]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    169\u001b[0m     MLD_li\u001b[38;5;241m.\u001b[39mappend(MLD)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39massign_coords(MLD\u001b[38;5;241m=\u001b[39m(dim1, MLD_li))\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/Argo_Nov_24/lib/python3.12/site-packages/xarray/core/dataarray.py:1479\u001b[0m, in \u001b[0;36mDataArray.isel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;66;03m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# lists, or zero or one-dimensional np.ndarray's\u001b[39;00m\n\u001b[0;32m-> 1479\u001b[0m variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m indexes, index_variables \u001b[38;5;241m=\u001b[39m isel_indexes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxindexes, indexers)\n\u001b[1;32m   1482\u001b[0m coords \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/Argo_Nov_24/lib/python3.12/site-packages/xarray/core/variable.py:993\u001b[0m, in \u001b[0;36mVariable.isel\u001b[0;34m(self, indexers, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    990\u001b[0m indexers \u001b[38;5;241m=\u001b[39m drop_dims_from_indexers(indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims, missing_dims)\n\u001b[1;32m    992\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(indexers\u001b[38;5;241m.\u001b[39mget(dim, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/Argo_Nov_24/lib/python3.12/site-packages/xarray/core/variable.py:764\u001b[0m, in \u001b[0;36mVariable.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a new Variable object whose contents are consistent with\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03mgetting the provided key from the underlying data.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03marray `x.values` directly.\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    763\u001b[0m dims, indexer, new_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broadcast_indexes(key)\n\u001b[0;32m--> 764\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mas_indexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_order:\n\u001b[1;32m    766\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(data, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_order)), new_order)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/Argo_Nov_24/lib/python3.12/site-packages/xarray/core/indexing.py:1337\u001b[0m, in \u001b[0;36mNumpyIndexingAdapter.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1336\u001b[0m     array, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexing_array_and_key(key)\n\u001b[0;32m-> 1337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "ds = prf.get_box(box3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign_attrs({\"Fetched_uri\":''})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(\"/swot/SUM05/amf2288/sync-boxes/lon:({},{})_lat:({},{})_ds_z.nc\".format(NW[0],NW[1],NW[2],NW[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_1x = [-10,0,-10,0,0,2000]   #error\n",
    "box_1y = [-10,0,-20,-10,0,2000] #done\n",
    "box_1z = [-10,0,-30,-20,0,2000] #done\n",
    "box_1a = [-10,0,-40,-30,0,2000] #done\n",
    "box_1b = [-10,0,-50,-40,0,2000] #done\n",
    "box_1c = [-10,0,-60,-50,0,2000] #done\n",
    "box_1d = [-10,0,-70,-60,0,2000] #done\n",
    "box_long=[-10,-8,-70,-10,0,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot_box([box_1x,box_1y,box_1z,box_1a,box_1b,box_1c,box_1d,box_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z=ff.get_box(box_long,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['N_PROF'].attrs = {'name':'Fetched Profile Number'}\n",
    "ds['PRES_INTERPOLATED'].attrs = {'name':'Interpolated Pressure', 'units':'dbar', 'notes':'interpolated to standard pressure grid of 2m'}\n",
    "\n",
    "ds['N_PROF_NEW'].attrs = {'name':'Sequential Profile Number'}\n",
    "ds['LATITUDE'].attrs = {'name':'Latitude', 'units':'°N'}\n",
    "ds['LONGITUDE'].attrs = {'name':'Longitude', 'units':'°E'}\n",
    "ds['month'].attrs = {'name':'Month'}\n",
    "ds['year'].attrs = {'name':'Year'}\n",
    "ds['MLD'].attrs = {'name':'Mixed Layer Depth', 'units':'m'}\n",
    "\n",
    "ds['CT'].attrs = {'name':'Conservative Temperature', 'units':'°C'}\n",
    "ds['SA'].attrs = {'name':'Absolute Salinity', 'units':'g kg-1'}\n",
    "ds['SIG0'].attrs = {'name':'Potential Density', 'units':'kg m-1', 'notes':'referenced to 0 dbar'}\n",
    "ds['SPICE'].attrs = {'name':'Spiciness', 'units':'kg m-1', 'notes':'referenced to 0 dbar'}                  \n",
    "ds = ds.assign_attrs({\"Fetched_uri\":''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ds_z.LONGITUDE, ds_z.LATITUDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z.to_netcdf(\"202206_boxes/lon:({},{})_lat:({},{})_ds_z.nc\".format(box_long[0],box_long[1],box_long[2],box_long[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erddap_loader=ArgoDataFetcher(src'erddap',parallel=True,progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaze_loader=ArgoDataFetcher(src='gdac',ftp='https://data-argo.ifremer.fr/',parallel=True,progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_loader=ArgoDataFetcher(src='gdac',ftp='ftp://usgodae.org/pub/outgoing/argo',parallel=True,progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=argo_loader.region(box_1x)\n",
    "print('loading points complete')\n",
    "ds=ds.to_xarray()\n",
    "print('to xarray complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.argo.teos10(['CT','SA','SIG0'])\n",
    "ds=ds.argo.point2profile()\n",
    "print('point to profile complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_box(box,sample_min):\n",
    "    '''Takes latitude/longitude/depth data and a sample rate and returns an xarray with CT, SA, SIG0, and SPICE interpolated to a pressure grid of 2m. \n",
    "    \n",
    "    box: lat/lon in the form: box=[lon_min, lon_max, lat_min, lat_max, depth_min, depth_max]\n",
    "    sample_min: minimum sample rate [m]\n",
    "    '''\n",
    "    \n",
    "    ds=argo_loader.region(box)\n",
    "    print('loading points complete')\n",
    "    \n",
    "    ds=ds.to_xarray()\n",
    "    print('to xarray complete')\n",
    "    \n",
    "    ds=ds.argo.teos10(['CT','SA','SIG0'])\n",
    "    ds=ds.argo.point2profile()\n",
    "    print('point to profile complete')\n",
    "    \n",
    "    ds_interp=get_ds_interp(ds,0,2000,sample_min)\n",
    "    print('interpolation complete')\n",
    "    \n",
    "    ds_interp['SPICE'] = gsw.spiciness0(ds_interp.SA,ds_interp.CT).rename('SPICE')\n",
    "    print('adding spice complete')\n",
    "        \n",
    "    return ds_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_get_box(box_medi,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=argo_loader.region(box_medi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.PSAL.dropna('N_POINTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0,len(ds.N_PROF)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "methods are also functions, but specific to the class they're applied to\n",
    "class (animals) --> subclass (dogs) --> object (golden retriever)\n",
    "open_mfdataset() from xarray, for opening data in multiple netcdfs\n",
    "1) write loop opening all files yourself, check for PSAL, if not there remove file\n",
    "2) go into open_mfdataset() and see if there's a loop there to exploit instead\n",
    "\n",
    "locally install repo (pip install -e??)\n",
    "uses the code from the local repo instead\n",
    "then make changes to repo: print statement in part of the code we think is happening\n",
    "then work on loops, etc.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xarray(self, errors: str = \"ignore\"):\n",
    "        \"\"\" Load Argo data and return a :class:`xarray.Dataset`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        errors: str, default='ignore'\n",
    "            Define how to handle errors raised during data URIs fetching:\n",
    "\n",
    "                - 'ignore' (default): Do not stop processing, simply issue a debug message in logging console\n",
    "                - 'silent':  Do not stop processing and do not issue log message\n",
    "                - 'raise': Raise any error encountered\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`xarray.Dataset`\n",
    "        \"\"\"\n",
    "        if (\n",
    "            len(self.uri) > 50\n",
    "            and isinstance(self.method, str)\n",
    "            and self.method == \"sequential\"\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Found more than 50 files to load, this may take a while to process sequentially ! \"\n",
    "                \"Consider using another data source (eg: 'erddap') or the 'parallel=True' option to improve processing time.\"\n",
    "            )\n",
    "        elif len(self.uri) == 0:\n",
    "            raise DataNotFound(\"No data found for: %s\" % self.indexfs.cname)\n",
    "\n",
    "        # Download data:\n",
    "        ds = self.fs.open_mfdataset(\n",
    "            self.uri, #list of all float files?\n",
    "            method=self.method,\n",
    "            concat_dim=\"N_POINTS\",\n",
    "            concat=True,\n",
    "            preprocess=self._preprocess_multiprof,\n",
    "            progress=self.progress,\n",
    "            errors=errors,\n",
    "            open_dataset_opts={'xr_opts': {'decode_cf': 1, 'use_cftime': 0, 'mask_and_scale': 1}},\n",
    "        )\n",
    "\n",
    "        # Data post-processing:\n",
    "        ds[\"N_POINTS\"] = np.arange(\n",
    "            0, len(ds[\"N_POINTS\"])\n",
    "        )  # Re-index to avoid duplicate values\n",
    "        ds = ds.set_coords(\"N_POINTS\")\n",
    "        ds = ds.sortby(\"TIME\")\n",
    "\n",
    "        # Remove netcdf file attributes and replace them with simplified argopy ones:\n",
    "        ds.attrs = {}\n",
    "        if self.dataset_id == \"phy\":\n",
    "            ds.attrs[\"DATA_ID\"] = \"ARGO\"\n",
    "        if self.dataset_id == \"bgc\":\n",
    "            ds.attrs[\"DATA_ID\"] = \"ARGO-BGC\"\n",
    "        ds.attrs[\"DOI\"] = \"http://doi.org/10.17882/42182\"\n",
    "        ds.attrs[\"Fetched_from\"] = self.server\n",
    "        ds.attrs[\"Fetched_by\"] = getpass.getuser()\n",
    "        ds.attrs[\"Fetched_date\"] = pd.to_datetime(\"now\", utc=True).strftime(\"%Y/%m/%d\")\n",
    "        ds.attrs[\"Fetched_constraints\"] = self.cname()\n",
    "        if len(self.uri) == 1:\n",
    "            ds.attrs[\"Fetched_uri\"] = self.uri[0]\n",
    "        else:\n",
    "            ds.attrs[\"Fetched_uri\"] = \";\".join(self.uri)\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_z.SIG0.values.min(), ds_z.SIG0.values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_grid = np.linspace(26.6, 28, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rho = df.interpolate2density_prof(ds_z, rho_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rho.to_netcdf(\"202206_boxes/lon:({},{})_lat:({},{})_ds_rho.nc\".format(box[0],box[1],box[2],box[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load by float (coordinate: distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_ID = 6901265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z = ff.get_float(float_ID, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z = ds_z.assign_attrs({\"Fetched_uri\":''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z.to_netcdf(\"202206_floats/float_ID:({})_ds_z.nc\".format(float_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_z.SIG0.values.min(), ds_z.SIG0.values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_grid = np.linspace(26.4, 28, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rho = df.interpolate2density_prof(ds_z, rho_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rho.to_netcdf(\"202206_floats/float_ID:({})_ds_rho.nc\".format(float_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Boxes by Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = np.arange(-180,181,4)\n",
    "lats = np.arange(-90,91,4)\n",
    "\n",
    "boxes=[]\n",
    "\n",
    "for n in range(0,len(lons)-1):\n",
    "    for m in range(0,len(lats-1)):\n",
    "        lon_min = lons[n]\n",
    "        lon_max = lons[n+1]\n",
    "        lat_min = lats[n]\n",
    "        lat_max = lats[n+1]\n",
    "        box_n = np.array([lon_min, lon_max, lat_min, lat_max])\n",
    "        boxes.append(box_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "lons[n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argopy import IndexFetcher as ArgoIndexFetcher\n",
    "\n",
    "box_bad=[-15,-14,36.5,37]#, '2000-01-01', '2021-06'] # need to remove pressure poinds because the region function for this index fetcher seems a bit different.\n",
    "\n",
    "idx = ArgoIndexFetcher(src='gdac',dataset='phy',mode='standard',\n",
    "                            ftp=\"/swot/SUM05/dbalwada/202203-ArgoData\").region(box_bad).load()\n",
    "idx.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_IDs = [1900041,1900749,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Argo_Nov_24]",
   "language": "python",
   "name": "conda-env-Argo_Nov_24-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
